# -*- coding: utf-8 -*-
"""Datamining_CA_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JWnpgoyeVZ-cua-j8gizM7g-tE930fXi
"""

#importing library
import pandas as pd
from pandas import read_csv, get_dummies, DataFrame,Series
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

#loading the dataset
data = pd.read_csv('/content/healthcare_dataset.csv')

#to know the data info
data.info()

#to check the null value
data.isnull().sum()

"""to check value counts in colunms"""

data['Blood Type'].value_counts()

data['Medical Condition'].value_counts()

data['Doctor'].value_counts()

data['Hospital'].value_counts()

data['Insurance Provider'].value_counts()

data['Room Number'].value_counts()

data['Admission Type'].value_counts()

data['Medication'].value_counts()

data['Test Results'].value_counts()

"""to drop unwanted colunms"""

data_to_drop=['Name','Doctor',
              'Hospital','Date of Admission','Room Number',
              'Discharge Date']

data_cleaned=data.drop(data_to_drop,axis=1)

data_cleaned.info()

"""converting gender colunm into 1 and 2 beacuse it is categorical values to numeric values"""

data_cleaned['Gender']=data_cleaned['Gender'].map({'Male':1,'Female':0})

data_cleaned['Gender']

"""converting numberical values although it 3 values like normal,abnormal and Inconclusive we covert it into two normal and abnormal"""

data_cleaned['Test Results'] = data_cleaned['Test Results'].apply(lambda x: 0 if x == 'Normal' else 1)

data_cleaned['Test Results']

data_cleaned.info()

"""taking dummies of objects"""

data_cleaned=get_dummies(data_cleaned,['Blood Type','Medical Condition',
                                       'Insurance Provider','Admission Type','Medication'])

data_cleaned['Test Results'].value_counts()

"""dropping test result indo y data and taking balance colunms as X data"""

X=data_cleaned.drop('Test Results',axis=1)
Y=data_cleaned['Test Results']

Y.value_counts()

"""#decsion tree"""

from sklearn import tree#importing tree

X_scaled=StandardScaler().fit_transform(X)
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1)#spliting train and test 70% and 30% respectively
X_train, Y_train =SMOTE(random_state=1).fit_resample(X_train, Y_train)#smoting the data because the data is note balanced
dec_tree= tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)#giving tree depth as 5
dec_tree.fit(X_train,Y_train)

Y_pred=dec_tree.predict(X_test)

Accuracy= metrics.accuracy_score(Y_test,Y_pred)
Recall= metrics.recall_score(Y_test,Y_pred)
Precision= metrics.precision_score(Y_test,Y_pred)
print('Accuracy:', Accuracy)
print('Recall:', Recall)
print('Precision:', Precision)
f1 = f1_score(Y_test, Y_pred)
print("F1-Score:", f1)

conf_mat= metrics.confusion_matrix(Y_test,Y_pred)

conf_mat

"""#graph for basic dt

Confusion Matrix Heatmap
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Basic Decision Tree')
plt.show()

"""Feature Importance Bar Plot"""

feat_importance = pd.Series(dec_tree.feature_importances_, index=X.columns)
feat_importance = feat_importance.sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=feat_importance.values, y=feat_importance.index)
plt.title("Top 10 Feature Importances - Basic Decision Tree")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

"""#Tunned DT"""

from sklearn.model_selection import GridSearchCV

dec_tree_gs = tree.DecisionTreeClassifier(criterion='entropy')
depth_gs = {'max_depth':[5,10,15,20,25,30,35,40,45,46,47,48,49,50,51,52,53,54,55,60]}#to check which depth is best for this model
grid_search_gs = GridSearchCV(estimator=dec_tree_gs, param_grid=depth_gs, scoring='precision', cv=5)
grid_search_gs.fit(X_train, Y_train)
best_depth_gs = grid_search_gs.best_params_
print(best_depth_gs)

from sklearn.model_selection import GridSearchCV

dec_tree_gs_final = tree.DecisionTreeClassifier(criterion='entropy', max_depth=48)#got as 48 so we give 48 as the best tree
dec_tree_gs_final.fit(X_train, Y_train)

Y_pred_gs = dec_tree_gs_final.predict(X_test)
Accuracy_gs = metrics.accuracy_score(Y_test, Y_pred_gs)
Recall_gs = metrics.recall_score(Y_test, Y_pred_gs)
Precision_gs = metrics.precision_score(Y_test, Y_pred_gs)
print('Accuracy:', Accuracy_gs)
print('Recall:', Recall_gs)
print('Precision:', Precision_gs)
f1_gs = f1_score(Y_test, Y_pred_gs)
print("F1-Score:", f1_gs)
conf_mat_gs = metrics.confusion_matrix(Y_test, Y_pred_gs)
print(conf_mat_gs)

"""#graph for Tunned DT"""

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat_gs, annot=True, fmt='d', cmap='YlGnBu', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - GridSearch Decision Tree')
plt.show()

feat_importance_gs = pd.Series(dec_tree_gs_final.feature_importances_, index=X.columns)
feat_importance_gs = feat_importance_gs.sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=feat_importance_gs.values, y=feat_importance_gs.index)
plt.title("Top 10 Feature Importances - GridSearch Decision Tree")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#random forest"""

from sklearn.ensemble import RandomForestClassifier

from sklearn import ensemble
rf_model = RandomForestClassifier(n_estimators=5, criterion='entropy', max_features=None, random_state=1)#giving intial estimators as 5
rf_model.fit(X_train, Y_train)

Y_pred_rf = rf_model.predict(X_test)

Accuracy_rf = accuracy_score(Y_test, Y_pred_rf)
Recall_rf = recall_score(Y_test, Y_pred_rf)
Precision_rf = precision_score(Y_test, Y_pred_rf)
f1_rf = f1_score(Y_test, Y_pred_rf)

print("Random Forest Results")
print("Accuracy:", Accuracy_rf)
print("Recall:", Recall_rf)
print("Precision:", Precision_rf)
print("F1-Score:", f1_rf)
conf_mat_rf = confusion_matrix(Y_test, Y_pred_rf)
print("Confusion Matrix:\n", conf_mat_rf)

"""#graphs for RF"""

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat_rf, annot=True, fmt='d', cmap='Greens', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

feat_importance_rf = pd.Series(rf_model.feature_importances_, index=X.columns)
feat_importance_rf = feat_importance_rf.sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=feat_importance_rf.values, y=feat_importance_rf.index)
plt.title("Top 10 Feature Importances - Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#Tunned RF"""

rf_gs = RandomForestClassifier(criterion='entropy', max_features=None, random_state=1)
param_grid_rf = {'n_estimators': [200,300]}#to check the best estimators for that we have checked 100,150,200,250,300 but two at a time because it is taking long time
grid_search_rf = GridSearchCV(estimator=rf_gs, param_grid=param_grid_rf, scoring='precision', cv=3)
grid_search_rf.fit(X_train, Y_train)

best_rf_params = grid_search_rf.best_params_
print("Best n_estimators:", best_rf_params)

rf_model_gs = RandomForestClassifier(n_estimators=best_rf_params['n_estimators'], criterion='entropy', max_features=None, random_state=1)
#giving the best estimator as 200 but it is automattically updated from the above code
rf_model_gs.fit(X_train, Y_train)

from sklearn.metrics import f1_score

Y_pred_rf_gs = rf_model_gs.predict(X_test)
Accuracy_rf_gs = accuracy_score(Y_test, Y_pred_rf_gs)
Recall_rf_gs = recall_score(Y_test, Y_pred_rf_gs)
Precision_rf_gs = precision_score(Y_test, Y_pred_rf_gs)
f1_rf_gs = f1_score(Y_test, Y_pred_rf_gs)

print("Tuned Random Forest Results")
print("Accuracy:", Accuracy_rf_gs)
print("Recall:", Recall_rf_gs)
print("Precision:", Precision_rf_gs)
print("F1-Score:", f1_rf_gs)
conf_mat_rf_gs = confusion_matrix(Y_test, Y_pred_rf_gs)
print("Confusion Matrix:\n", conf_mat_rf_gs)

"""#Tunned RF graph"""

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat_rf_gs, annot=True, fmt='d', cmap='Oranges', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Tuned Random Forest')
plt.show()

feat_importance_rf_gs = pd.Series(rf_model_gs.feature_importances_, index=X.columns)
feat_importance_rf_gs = feat_importance_rf_gs.sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=feat_importance_rf_gs.values, y=feat_importance_rf_gs.index)
plt.title("Top 10 Feature Importances - Tuned Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#logistic Regression"""

from sklearn.linear_model import LogisticRegression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

logreg_model = LogisticRegression(max_iter=1000)#giving intial value as 1000
logreg_model.fit(X_train, Y_train)

Y_pred_logreg = logreg_model.predict(X_test)

Accuracy_logreg = accuracy_score(Y_test, Y_pred_logreg)
Recall_logreg = recall_score(Y_test, Y_pred_logreg)
Precision_logreg = precision_score(Y_test, Y_pred_logreg)
f1_logreg = f1_score(Y_test, Y_pred_logreg)

print("Logistic Regression Results")
print("Accuracy:", Accuracy_logreg)
print("Recall:", Recall_logreg)
print("Precision:", Precision_logreg)
print("F1-Score:", f1_logreg)
conf_mat_logreg = confusion_matrix(Y_test, Y_pred_logreg)
print("Confusion Matrix:\n", conf_mat_logreg)

"""#graph for logistic"""

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat_logreg, annot=True, fmt='d', cmap='Purples', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

coefficients = pd.Series(logreg_model.coef_[0], index=X.columns)
coefficients = coefficients.sort_values(key=abs, ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=coefficients.values, y=coefficients.index)
plt.title("Top 10 Feature Coefficients - Logistic Regression")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#Tunned LogisticRegression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

param_grid_logreg = {
    'C': [0.1, 1],
    'penalty': ['l1', 'l2']
}
#tuning the logistic for better perfomance

logreg_gs = LogisticRegression(max_iter=1000)
grid_search_logreg = GridSearchCV(estimator=logreg_gs, param_grid=param_grid_logreg, scoring='f1', cv=5)
grid_search_logreg.fit(X_train, Y_train)

print("Best Parameters (LogReg):", grid_search_logreg.best_params_)

best_logreg_model = grid_search_logreg.best_estimator_
Y_pred_logreg_gs = best_logreg_model.predict(X_test)#giving the best Best Parameters (LogReg): {'C': 1, 'penalty': 'l2'} which is auto matically upataing the code

Accuracy_logreg_gs = accuracy_score(Y_test, Y_pred_logreg_gs)
Recall_logreg_gs = recall_score(Y_test, Y_pred_logreg_gs)
Precision_logreg_gs = precision_score(Y_test, Y_pred_logreg_gs)
f1_logreg_gs = f1_score(Y_test, Y_pred_logreg_gs)

print("GridSearch Logistic Regression Results")
print("Accuracy:", Accuracy_logreg_gs)
print("Recall:", Recall_logreg_gs)
print("Precision:", Precision_logreg_gs)
print("F1-Score:", f1_logreg_gs)
conf_mat_logreg_gs = confusion_matrix(Y_test, Y_pred_logreg_gs)
print("Confusion Matrix:\n", conf_mat_logreg_gs)

"""#graph for loistic"""

plt.figure(figsize=(6,4))
sns.heatmap(conf_mat_logreg_gs, annot=True, fmt='d', cmap='BuPu', xticklabels=["Normal", "Abnormal"], yticklabels=["Normal", "Abnormal"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Tuned Logistic Regression')
plt.show()

coefficients_logreg_gs = pd.Series(best_logreg_model.coef_[0], index=X.columns)
coefficients_logreg_gs = coefficients_logreg_gs.sort_values(key=abs, ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(x=coefficients_logreg_gs.values, y=coefficients_logreg_gs.index)
plt.title("Top 10 Feature Coefficients - Tuned Logistic Regression")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#final model comparison bar chart"""

import matplotlib.pyplot as plt
import numpy as np

# Define the models
models = ['Basic DT', 'Tuned DT', 'Random Forest', 'Tuned RF', 'LogReg', 'Tuned LogReg']
x = np.arange(len(models))
width = 0.2

# Create lists to store the evaluation metrics for each model
accuracy_vals = [Accuracy, Accuracy_gs, Accuracy_rf, Accuracy_rf_gs, Accuracy_logreg, Accuracy_logreg_gs]
precision_vals = [Precision, Precision_gs, Precision_rf, Precision_rf_gs, Precision_logreg, Precision_logreg_gs]
recall_vals = [Recall, Recall_gs, Recall_rf, Recall_rf_gs, Recall_logreg, Recall_logreg_gs]
f1_vals = [f1, f1_gs, f1_rf, f1_rf_gs, f1_logreg, f1_logreg_gs]

# Plotting the bar chart
plt.figure(figsize=(12, 6))
plt.bar(x - 1.5*width, accuracy_vals, width, label='Accuracy', color='steelblue')
plt.bar(x - 0.5*width, precision_vals, width, label='Precision', color='darkorange')
plt.bar(x + 0.5*width, recall_vals, width, label='Recall', color='forestgreen')
plt.bar(x + 1.5*width, f1_vals, width, label='F1-Score', color='crimson')

plt.xticks(x, models, rotation=30)
plt.ylabel('Scores')
plt.ylim(0, 1)
plt.title('Comparison of Metrics Across All Models')
plt.legend()
plt.tight_layout()
plt.show()

"""#ROC Curve – Compare All Models final comparison"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

models = {
    'Basic DT': dec_tree,
    'Tuned DT': dec_tree_gs_final,
    'Random Forest': rf_model,
    'Tuned RF': rf_model_gs,
    'LogReg': logreg_model,
    'Tuned LogReg': best_logreg_model
}

plt.figure(figsize=(10, 8))
for name, model in models.items():
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(Y_test, y_prob)  # Use Y_test instead of y_test
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison - All Models")
plt.legend()
plt.grid(True)
plt.show()

"""additional improvement

More Interpretability → SHAP, PDP, LIME
"""

!pip install lime --quiet

import lime
import lime.lime_tabular
import numpy as np

explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train,
    feature_names=X.columns,
    class_names=["Normal", "Abnormal"],
    discretize_continuous=True
)

row_index = 5
exp = explainer.explain_instance(X_test[row_index], rf_model_gs.predict_proba)
exp.show_in_notebook()